\documentclass[10pt, onecolumn]{article}

\usepackage[utf8]{inputenc}
%\renewcommand*{\familydefault}{\sfdefault}

\usepackage{amsmath, amssymb}

\usepackage{graphicx}
\usepackage{fullpage}

\newcommand{\U}{\mathcal{U}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\newx}{{x^\star}}
\newcommand{\newy}{y^\star}
\newcommand{\answer}{h^\star}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\D}{\mathcal{D}}


\usepackage{lineno}

%\linenumbers

\usepackage[numbers, square, sort]{natbib}

\usepackage{framed}

\parindent0pt
\parskip5pt


\begin{document}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\title{Active learning with cautious oracle that provides relative answers}
\author{A1, A2}
\maketitle

\section{Basis model for active learning}
Using the language of the 2010 survey by Settles.

Task: Train a classifier semi-supervised manner. We have a set $\L$ of labeled training samples, and we want to improve the classifier by querying an annotator for some more.

The scenario: Pool-based sampling. We have a pool $\U$ of unlabeled samples which we can use for querying the oracle for their label.

Query strategy: uncertainty sampling using entropy as query informativeness.

\subsection{Cautious oracles}
The annotator gives a label-probability vector instead of a single label (oracle) or random label (noisy oracle).

The idea: Annotator is modelled as the probability vector producer. We use the Dirichlet-distribution to model the annotator.

Model: Let $\D=\{(y,x)\}$ denote a set of items, where $y\in C$ with $|C|=m$ denote labels for the items, and $x\in \X\subseteq \R^p$ denote other features (covariates). Write for any $x$ the correct label $y_x$. 

Start with a training set of correcly labeled items $\L=\{(y_i; x_i)\}$. Denote $\U=\D\setminus \L$. The classifier model

\begin{eqnarray}
y | x, \theta &\sim & p(y| x, \theta)\\
\theta & \sim & p(\theta)
\end{eqnarray}

learns the $\theta$ from the $\L$. Active learning is about increasing the model fitness  by querying most informative $x$ to get $y_x$ so as optimally increase upon $\L$ with minimum number of queries. The basic setting assumes an \emph{oracle}, an external annotator which provides the correct label for any query $\newx\in \U$ and which we then use to enhance the estimate of $\theta$.

Now define a \emph{cautious oracle}: when queried for a label for $\newx\in \U$ the cautious oracle's answer is a distribution over $C$ instead of the correct label:

\begin{eqnarray}
\answer & \sim & p(\answer | \newx, \alpha), \quad \answer \in \H=\{h\in \R_+: 0<\sum_{c\in C} h_c<\infty\}\\
p(\newy=c' | \newx) &=&h_{c'}/\sum h_{c} \\
\alpha & \sim & p(\alpha)
\end{eqnarray}

We get the oracle from this by setting $$p(h|\newx,\alpha)=1(h_{y_\newx}>0, h_{c}\neq 0\  \forall\ c\neq y_\newx).$$ In other words, the active learning maps queries with functions
\begin{eqnarray}
\mathrm{oracle} \quad f_o(\newx) & = & y_\newx \\
\mathrm{cautious\ oracle} \quad f_{co}(\newx) & = & h_\newx
\end{eqnarray}

How do we use the cautious oracle for active learning? The oracle is used in the following algorithmic manner:
\begin{enumerate}
\item Compute the informativeness $I(x):=I(x | \theta, \L)$ of each $x\in \U$
\item Augment $\L:= \L \cup \{(f_o(\newx), \newx)\}$ where $\newx = argmax\ I(x)$.
\item Check some stopping rule.
\end{enumerate}

The cautious oracle case suggest two approaches to the algorithm, of which both lead to the same probabilistic model. First is the 'heuristic' realisation approach, where we sample the label from the distribution provided by the oracle, viz.
\begin{enumerate}
\item[2'.] Augment $\L:= \L \cup \{(\tilde{y}_\newx, \newx)\}$ where $\newx = argmax\ I(x)$ and $\tilde{y}_\newx\sim h_\newx$.
\end{enumerate}
This leads to a new source of noise in $\L$, which then is handled by the augmenting the classifier model. 

The other option is to change the structure of the data. Let again $\L_t$ be the queried set by now consisting of pairs $(h_x,x)$. Then 
\begin{itemize}
\item[2''.] Augment $\L:=\L\cup \{(h_\newx, \newx)\}$ where $\newx=argmax\ I(x)$.
\end{itemize} 

If $\L_0$ is the (clean) training set, and $\L_t$ is the queried set after $t$ iterations, we add to the model (1)
\begin{eqnarray}
\tilde p(h_x|x, \theta) &=& E_{\tilde y|h_x}p(\tilde y|x, \theta) \qquad (h_x, x)\in \L_t
\end{eqnarray}

This is then used for inferring $\theta$, and for deriving new  values for $I(x)$.

\subsection{Example 1}
Let the classifier be naive Bayes, i.e. $$p(y=c|\theta, x) \propto p(c|\theta_c)\prod p(x_k |\theta_c)$$ where $\theta_c$ are the parameters corresponding to class $c$. 

Let the informativeness measure be the entropy,
$$I(x):=E_{\theta|\L}\sum_c p(y=c|x, \theta)\log p(y=c|x, \theta).$$



\section{Model for the cautious oracle}
Can we infer $$p(h|x, \alpha)?$$ 

\end{document}







